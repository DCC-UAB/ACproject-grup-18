SESSIÓ 2:

1. Quin tipus de dades conté el dataset? Com és el nostre dataset i que descriu? Quina és la seva mida inicial?
2. Hi ha valors nulls o errònis?
    No hi ha valors nulls o errònis en el dataset.
3. Hi ha columnes redundants?
4. Quina distribució tenen les nostres dades? Podem veure tendències?
5. Una vegada carregat el dataset, hem de filtrat? Si és així, quins són els criteris?
6. Cal pujar el dataset al GitHub?
    No, ja que al ser un arxiu massa gran no es pot pujar directament al repositori, cal que cada membre el tingui descarregat.
7. Quins models recomanadors podem aplicar per aquest dataset?

SESSIÓ 3 i 4:

1. Com dividim les dades?
2. Quines són els millors paràmetres pel model User-to-User? I per l'Item-to-Item?
3. Quina diferència hi ha entre la distància de Pearson i Cosinus?
4. Pels models user i item analitzem el nombre de veïns, quin és l'equivalent per SVD?
5. Com podem comparar els diferents models? Quines mètriques utilitzem?
6. És l'error igual per tots les valoracions? O hi ha valoracions on els models s'equivoquen més?
7. Són els temps d'execució dels models iguals o hi ha diferències?
8. Què passa quan tenim menys dades, és a dir, quan reduïm el tamany del dataset amb el que treballem?
9. Hi ha items o usuaris que coincideixen en les recomanacions? Quins són els items que més es recomanen per cada model?
10. Aplicar la tècnica de cross-validation canvia els resultats?